---
title: 개굴캐글개굴 데이터 전처리
date: 2023-10-29 21:30:00 +0900
categories: [AI,Tab_Study]
tags: [AI,Tab_Study]
---

![](https://velog.velcdn.com/images/acadias12/post/358c7f7f-00ec-4dd7-99a4-183615838474/image.png)

Tab_study를 진행하면서 배운 데이터 전처리에 대해 정리해보려한다.

## 훈련 세트와 테스트 세트 분류

데이터 셋을 훈련시키고 평가하기 위해 훈련 세트와 테스트 셋을 분류한다.

### 데이터를 섞지 않은 경우

```python
import numpy as np
input_arr = np.array(fish_data)
target_arr = np.array(fish_target)

```
이렇게 입력 값과 결과 값이 주어진 것을 지도학습이라고 하는데, 만약 위의 코드처럼 데이터 셋들이 섞이지(shuffle) 않는 경우 정확한 테스트가 불가능 하다. 

### 데이터를 섞은 경우

```python
import numpy as np
np.random.shuffle(fist_data)
np.random.shuffle(fist_target)
input_arr = np.array(fish_data)
target_arr = np.array(fish_target)
```
이런 식으로 데이터들을 섞어준다면 모델들이 학습을 원활하게 할 수 있다.

## 데이터 전처리
모델들을 학습시키기 위해서는 불필요하거나, 학습을 저해시키는 데이터들을 처리 해야한다.

### 데이터 셋 준비

```python
import numpy as np
fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 
                31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 
                35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 
                10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 
                500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 
                700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 
                7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]
fish_data = np.column_stack((fish_length, fish_weight))
fish_target = np.concatenate((np.ones(35), np.zeros(14)))
```

### 데이터 나누기

```python
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, stratify=fish_target random_state=42)
```
데이터 셋을 나눠주는 것은  sklearn에서 제공하고있는데 위의 코드처럼 train_test_split을 통해 데이터 셋을 나눠준다.
stratify는 분류 모델에서 타겟 값이 골고루 섞이도록 테스트 셋을 나눠주는 역할을 하고, random_state는 동일한 값이 나오도록 random_seed를 설정하는 것이다.

### 결과 값 예측해보기

```python
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier()
kn.fit(train_input, train_target)
kn.score(test_input, test_target)
```

```
1.0
```

위의 코드를 통해 모델을 학습시키고 결과를 예측해보면 100%센트 확률로 물고기를 비교해내는 것을 확인할 수 있다.

```python
print(kn.predict([[25, 150]]))
```
```
[0.]
```
하지만 도미 데이터를 넣었을때 빙어 데이터로 예측하는 것을 볼 수 있다. 이 문제를 해결해보자.

```python
mean = np.mean(train_input, axis=0)
std = np.std(train_input, axis=0)

train_scaled = (train_input-mean)/std

new = ([25, 150]-mean)/std
```
문제를 해결하기 위해 학습데이터의 입력 값을 위의 코드로 표준 점수로 바꾸어보자.

```python
kn.fit(train_scaled, train_target)

test_scaled = (test_input-mean)/std

print(kn.score(test_scaled, test_target))
```
```
1.0
```
```
print(kn.predict([new]))
```
```
[1.]
```
표준 점수로 바꾸니 물고기를 잘 비교하는 것을 확인할 수 있다.

## 느낀점
데이터를 분류하고 전처리를 해보면서 학습 모델의 성능을 올리기 위해서는 단순히 학습시키는 것이 아니라 데이터들을 다루는 전처리 기술이 많이 중요하다는 것을 깨닫게 되었다. 나는 데이터들을 바라보는 시각을 넓혀야겠다고 생각했다.