---
title: 개굴캐글개굴 로지스틱 회귀
date: 2023-10-30 01:19:00 +0900
categories: [AI,Tab_Study]
tags: [AI,Tab_Study]
---

![](https://velog.velcdn.com/images/acadias12/post/358c7f7f-00ec-4dd7-99a4-183615838474/image.png)

Tab study에서 배운 로지스틱 회귀에 대해 정리해보려한다.

## 로지스틱 회귀

### 로지스틱 회귀란?
> 로지스틱 회귀는 수학을 사용하여 두 데이터 요인 간의 관계를 찾는 데이터 분석 기법이다.

### 로지스틱 회귀는 분류 모델인가? 회귀 모델인가?
로지스틱 회귀는 이름은 회귀이지만, 데이터의 결과가 특정 분류로 나뉘기 때문에 일종의 분류(classification) 모델이다.


### 시그모이드(sigmoid) 함수
![](https://velog.velcdn.com/images/acadias12/post/cc544f36-104a-4454-88b1-dafa96935937/image.png)

위의 사진처럼 시그모이드 출력 값으로 인해 양성 값인지 음성 값인지 분류할 수 있다. 시그모이드 함수 식은 $\sigma(z)=\frac1{1+e^-z}$이다. 쉽게 생각해서 시그모이드 함수 = 로지스틱 함수 라고 생각해도 될 것 같다.

### 로지스틱 함수 사용해보기

```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(C=20, max_iter = 1000)
lr.fit(train_scaled, train_target)
```
sklearn에서 제공하는 LogisticRegression함수를 사용할 수 있는데, max_iter는 반복횟수고 default는 100이다. C=20은 L2규제의 Alpha값이다. **기본적으로 로지스틱 회귀는 L2규제를 따른다**


## 경사하강법

### 경사하강법이란?
> 기울기가 낮은쪽으로 이동하는 것을 말한다. 즉, 손실함수가 최소가 되는 지점에 가까워 지도록 하강을 하는 것이다.

### 경사하강법의 Step  Size
![](https://velog.velcdn.com/images/acadias12/post/0b767d81-2965-41e5-a45b-d08c954e5752/image.png)

경사하강법의 Step을 크게할수록 걸리는 시간이 적어지겠지만, 손실함수의 최소가 되는 지점에서 멀어질 수 있다. 반대로 Step을 너무 적게 설정한다면, 손실함수의 최소가 되는 지점을 잘 찾을 수 있지만, 엄청난 시간이 걸리기 때문에 Step Size를 조절하는 것은 중요하다.

### 경사하강법의 반복횟수의 과대적합과 과소적합

경사하강법에서 반복횟수도 중요한데, 반복횟수(Epoch)의 설정에 따라 모델이 과대적합 혹은 과소적합이 될 수 있다. 왜냐하면 반복횟수를 적게 설정하면 당연히 모델의 학습은 불충분하므로 과소적합이 될 것이다. 반대로, 반복횟수를 너무 많이 설정하면 모델의 학습데이터에 과대 적합이 될 것이다. 따라서 반복횟수를 적절하게 설정하는 것이 중요하다.

### 경사하강법 코드로 확인해보기

```python
import numpy as np

sc = SGDClassifier(loss='log', random_state=42)
train_score = []
test_score = []
classes = np.unique(train_target)

for _ in range(0, 300):
    sc.partial_fit(train_scaled, train_target, classes = classes)
    train_score.append(sc.score(train_scaled, train_target))
    test_score.append(sc.score(test_scaled, test_target))
    
import matplotlib.pyplot as plt

plt.plot(train_score)
plt.plot(test_score)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()
```
![](https://velog.velcdn.com/images/acadias12/post/883bf253-e166-4d14-b087-4cc26c3bdf9d/image.png)

위의 사진처럼 반복횟수가 증가함에 따라 정확도도 증가하지만, 어느 순간부터 정확도가 감소하는 것을 확인할 수 있다. 우리는 조기 종료(early stopping)을 통해 최적의 반복횟수에서 멈춰 과대적합이 일어나는 것을 막을 수 있다.

## 느낀점
이번 스터디에서는 로지스틱 회귀와 경사하강법에 대해 배워보았다. 아직 수식에 대해 익숙하지 않아 어려움이 있지만, 경사하강법, 로지스틱 회귀 둘 다 많이 사용하는 것이므로 반복해서 공부해봐야겠다.